#!/usr/bin/env bash

# Shell style guide: https://google.github.io/styleguide/shell.xml

# ## (-) master and worker steps
# ## (*) worker specific steps
#
# - label nodes
# - disable scheduling
# - double ASG size
# - wait for 2x group
# - stop ASG actions
# - drain/terminate labelled nodes (sequentially)
# - resize ASG to original size
# - re-enable ASG actions

set -o errexit
set -o nounset
set -o pipefail

log() {
  echo -e "[$(date +'%Y-%m-%dT%H:%M:%S')]: ${*}" >&2
}

usage() {
  cat <<EOF
Usage: $0 -c <kube_context> -p <aws_profile> -s [retire_time_label] -r [role]
        -t [timeout]

  -s    Resume node rolling. Argument is retire_time label. If set, -r [role]
        must also be set
  -t    Node drain timeout. How long to wait for the node to drain before
        shutting it down (in seconds, default ${timeout}s)
EOF
} >&2

parse_opts() {
  local aws_profile=''

  # flags
  aws_opts=()
  kube_context=''
  role=''
  resume=''
  retire_time=$(date +"%Y-%m-%dT%H-%M-%SZ")
  timeout=600

  while getopts 'c:p:r:hs:t:' flag; do
    case "${flag}" in
      c) kube_context="${OPTARG}" ;;
      p) aws_profile="${OPTARG}" ;;
      r) role="${OPTARG}" ;;
      s) resume="${OPTARG}" ;;
      t) timeout="${OPTARG}" ;;
      h) usage && exit 0 ;;
      *) log "Unexpected option: ${flag}" && usage && exit 1 ;;
    esac
  done

  ### Validation
  if [[ -z "${kube_context}" ]]; then
    usage
    exit 1
  fi

  if [[ -n "${resume}" ]] && [[ -z "${role}" ]] ; then
    log "If you are resuming, you need to provide a role"
    usage
    exit 1
  fi

  if [[ -n "${resume}" ]]; then
      retire_time=${resume}
  fi

  if [[ -n "${aws_profile}" ]]; then
    if aws configure list --profile "${aws_profile}" &> /dev/null; then
      aws_opts=("--profile=${aws_profile}")
      aws "${aws_opts[@]}" sts get-caller-identity &>/dev/null \
        || (log "failed to get-caller-identity profile=${aws_profile}"; exit 1)
    else
      log "Invalid profile: ${aws_profile}"
      exit 1
    fi
  fi
}

checkdeps() {
  local missing_deps=""
  for d in "${@}"; do
    if ! command -v "${d}" &> /dev/null; then
      missing_deps+="${d} "
    fi
  done

  if [[ -n "${missing_deps}" ]]; then
    log "Missing dependencies: ${missing_deps}"
    exit 1
  fi
}

function retry() {
  local n=1
  local max=12
  local delay=8
  while true; do
    if "$@"; then
      break
    else
      if [[ $n -lt $max ]]; then
        ((n++))
        log "command failed: attempt=$n max=$max"
        sleep $delay;
      else
        log "the command has failed after $n attempts"
        exit 1
      fi
    fi
  done
}

label_for_cycling() {
  local role=$1
  local nodes
  nodes=$(retry kubectl --context="${kube_context}" get nodes -l "role=${role}" -o json | jq -r '.items[].metadata.name')
  log "${kube_context}: nodes=$(log "${nodes}" | wc -l) role=${role}"
  log "labelling for retirement: role=${role}"
  for node in ${nodes}; do
    retry kubectl --context="${kube_context}" label node "${node}" "retiring=${retire_time}" --overwrite=true
    retry kubectl --context="${kube_context}" cordon "${node}"
  done
}

kill_node() {
  local node=$1

  set +e
  time timeout "${timeout}" kubectl --context="${kube_context}" drain "${node}" --ignore-daemonsets --force --delete-local-data
  local rc=$?
  set -e
  if [[ ${rc} -eq 0 ]]; then
    log "drained successfully"
  elif [[ ${rc} -eq 124 ]]; then
    log "timeout reached, continuing: timeout=${timeout}"
  else
    log "kubectl drain error: rc=${rc}"
  fi

  local instance_id
  instance_id=$(retry aws "${aws_opts[@]}" --output=json ec2 describe-instances --filters "Name=network-interface.private-dns-name,Values=${node}" |\
    jq -r '.Reservations[].Instances[].InstanceId')
  log "aws terminating: node=${node} instance-id=${instance_id}"
  retry aws "${aws_opts[@]}" ec2 terminate-instances --instance-ids="${instance_id}"
}

upscale_asg() {
  local role=$1
  local asg_name=$2
  local asg_count=$3

  # - double ASG size
  retry aws "${aws_opts[@]}" --output=json autoscaling update-auto-scaling-group \
    --auto-scaling-group-name "${asg_name}" \
    --desired-capacity "$(( asg_count * 2 ))" \
    --max-size "$(( asg_count * 2 ))"

  # - wait for 2x group
  local count=0
  while [[ ${count} -lt ${asg_count} ]]; do
    sleep 32
    count=$(retry kubectl --context="${kube_context}" get nodes -l "role=${role}" -Lrole,retiring |\
      grep -v "${retire_time}" |\
      grep -v NotReady |\
      tail -n +2 |\
      wc -l)
    log "waiting for new nodes: actual=${count} expected=${asg_count} asg=\"${asg_name}\""
  done

  # - stop ASG actions
  ## auto-scaling processes:
  # https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html#process-types
  #
  # - Launch
  # - Terminate         # Do not suspend
  # - HealthCheck       # Do not suspend
  # - ReplaceUnhealthy
  # - AZRebalance
  # - AlarmNotification
  # - ScheduledActions
  # - AddToLoadBalancer # Do not suspend
  #
  # Suspends processes on the autoscaling group. As we drain and terminate
  # instances we don't want the ASG to spin up new ones as a replacement or to get
  # back to the desired capacity.
  #
  # We do however want the ASG to detect that we are terminating instances so
  # we want ASG to check the health of the instance and "Terminate" (remove from
  # the group)
  retry aws "${aws_opts[@]}" --output=json autoscaling suspend-processes \
    --auto-scaling-group-name "${asg_name}" --scaling-processes \
    "Launch" \
    "ReplaceUnhealthy" \
    "AZRebalance" \
    "AlarmNotification" \
    "ScheduledActions"
}

assert_asg_balance() {
  local role=$1

  # check that nodes are balanced across three AZs
  local nodes_per_zone
  nodes_per_zone=$(kubectl --context "${kube_context}" get nodes -l "role=${role}" --no-headers -ocustom-columns=':.metadata.labels.failure-domain\.beta\.kubernetes\.io\/zone' |\
      sort |\
      uniq -c)
  local npz
  readarray -t npz < <(echo "${nodes_per_zone}" | awk '{print $1}')
  if [ "${#npz[@]}" -ne 3 ]; then
      log "Expected nodes across three zones. Node distribution:\n$nodes_per_zone\nCannot proceed, exiting"
      exit 1
  fi
  # shellcheck disable=SC2252
  if [ "${npz[0]}" != "${npz[1]}" ] || [ "${npz[0]}" != "${npz[2]}" ] || [ "${npz[1]}" != "${npz[2]}" ]; then
      log "Nodes are not perfectly balanced across zones. Node distribution:\n$nodes_per_zone\nCannot proceed, exiting"
      exit 1
  fi
}

downscale_asg() {
  local asg_name=$1
  local asg_count=$2

  # - resize ASG to original size
  retry aws "${aws_opts[@]}" --output=json autoscaling update-auto-scaling-group \
    --auto-scaling-group-name "${asg_name}" --desired-capacity "${asg_count}" \
    --max-size "${asg_count}"

  # sleep some time to allow ASG to catch and see the "Terminating" instance
  sleep 64

  # wait for ASG to catch up and have the desired number of instances
  set +e
  local ic=0
  while [[ ${ic} -ne ${asg_count} ]]; do
    sleep 32
    ic=$(retry aws "${aws_opts[@]}" --output=json autoscaling describe-auto-scaling-groups \
      --auto-scaling-group-names "${asg_name}" |\
      jq -r -e '.AutoScalingGroups[0].Instances | length')
    log "waiting ASG to scale down: actual=${ic} desired=${asg_count} asg=\"${asg_name}\""
  done
  set -e

  # - re-enable ASG actions
  retry aws "${aws_opts[@]}" --output=json autoscaling resume-processes \
    --auto-scaling-group-name "${asg_name}" --scaling-processes \
    "Launch" \
    "ReplaceUnhealthy" \
    "AZRebalance" \
    "AlarmNotification" \
    "ScheduledActions"
}

drain_nodes() {
  local role=$1
  local retire_time=$2

  # - drain/terminate labelled nodes (sequentially)
  local old_nodes
  old_nodes=$(retry kubectl --context="${kube_context}" get nodes -l "role=${role},retiring=${retire_time}" -o json | jq -r '.items[].metadata.name')
  for old_node in ${old_nodes}; do
    kill_node "${old_node}"
  done
}

update() {
  local role=$1
  if [[ -z "${resume}" ]]; then
    label_for_cycling "${role}"
  fi

  local instance_address instance_id asgs asg_name asg_count
  while : ; do
    instance_address=$(retry kubectl --context="${kube_context}" get nodes -l "role=${role},retiring=${retire_time}" -o json | jq -r '.items[0].metadata.name')
    instance_id=$(retry aws "${aws_opts[@]}" --output=json ec2 describe-instances --filters "Name=network-interface.private-dns-name,Values=${instance_address}" |\
      jq -r '.Reservations[].Instances[].InstanceId')
    asgs=$(retry aws "${aws_opts[@]}" --output=json autoscaling describe-auto-scaling-groups)
    asg_name=$(echo "${asgs}" | jq -r ".AutoScalingGroups[] | select(.Instances[].InstanceId==\"${instance_id}\") | .AutoScalingGroupName")
    asg_count=$(echo "${asgs}" | jq -r ".AutoScalingGroups[] | select(.AutoScalingGroupName==\"${asg_name}\") | .MinSize")
    if [[ -z "${asg_name}" ]]; then
        log "could not discover the ASG name, a node might be terminating, sleeping and trying again..."
        sleep 60
    else
        break
    fi
  done
  log "asg_name: '${asg_name}' / asg_count: '${asg_count}'"

  if [[ -z "${resume}" ]]; then
    upscale_asg "${role}" "${asg_name}" "${asg_count}"

    assert_asg_balance "${role}"
  fi

  drain_nodes "${role}" "${retire_time}"

  downscale_asg "${asg_name}" "${asg_count}"
}

checkdeps jq kubectl aws timeout xargs readarray
parse_opts "${@}"

log "kube cluster: ${kube_context}"

if [[ -n "${role}" ]]; then
  update "${role}"
else
  update master
  update worker
fi

log "run: result=\"success\""
